# Quick Reference: hBehaveMAE Data Propagation

## One-Page Summary for Config: `--input_size 400 1 72 --q_strides 2,1,4;2,1,6 --stages 2 3 4`

### The Journey of Data Through hBehaveMAE

```
INPUT [B, 400, 72] â†’ Behavioral sequence
    â†“ Patch Embedding (Conv3d)
    
TOKENS [B, 4800, 96] â†’ 4,800 tokens, each covering 2Ã—1Ã—3 input elements
    â†“ Stage 0: Blocks 0-1 (Local Attention)
    
TOKENS [B, 4800, 96] â†’ Fine-grained features, local patterns
    â†“ Q-POOLING: stride=(2,1,4) â†’ Reduce by 8Ã—
    
TOKENS [B, 600, 192] â†’ 600 tokens, each covering 4Ã—1Ã—12 input elements
    â†“ Stage 1: Blocks 2-4 (Global Attention)
    
TOKENS [B, 600, 192] â†’ Mid-level features, action sequences
    â†“ Q-POOLING: stride=(2,1,6) â†’ Reduce by 12Ã—
    
TOKENS [B, 50, 384] â†’ 50 tokens, each covering 8Ã—1Ã—72 input elements
    â†“ Stage 2: Blocks 5-8 (Global Attention)
    
ENCODER OUTPUT [B, 50, 256] â†’ High-level features, complex activities
    â†“ Decoder (with mask tokens)
    
PREDICTIONS [B, 50, 288] â†’ Reconstruct masked regions
    â†“ Loss on masked tokens only
    
TRAINED MODEL â†’ Learns hierarchical behavioral representations
```

### Key Concepts Explained

#### ğŸ¯ What is a Token?
A **token** is a learned vector representation of a small spatiotemporal patch from your input data. 
- Initially: 4,800 tokens from patch embedding
- Each token starts covering 2 frames Ã— 3 width units
- Tokens become more abstract as they flow through stages

#### ğŸ”„ What is Q-Stride?
**Q-stride** controls token pooling in the attention mechanism:
- **Mechanism**: Max-pools queries (Q) while keeping keys (K) and values (V) at original resolution
- **Effect**: Reduces number of tokens, increases receptive field
- **Example**: q_stride=(2,1,4) reduces 4,800 tokens â†’ 600 tokens (8Ã— reduction)

#### ğŸ“¡ Receptive Field Growth
As tokens are pooled, their receptive field grows:
```
Stage 0: 2Ã—1Ã—3    = 6 input elements per token     (fine-grained)
Stage 1: 4Ã—1Ã—12   = 48 input elements per token    (mid-level)
Stage 2: 8Ã—1Ã—72   = 576 input elements per token   (high-level)
```
Final tokens see 96Ã— more input area than initial tokens!

### The Magic of Hierarchical Learning

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Lower Stages (Stage 0)                                      â”‚
â”‚  â€¢ Many tokens (4,800)                                       â”‚
â”‚  â€¢ Small receptive fields (2Ã—1Ã—3)                            â”‚
â”‚  â€¢ Local attention                                           â”‚
â”‚  â†’ Learns fine-grained patterns (individual movements)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Middle Stages (Stage 1)                                     â”‚
â”‚  â€¢ Fewer tokens (600)                                        â”‚
â”‚  â€¢ Medium receptive fields (4Ã—1Ã—12)                          â”‚
â”‚  â€¢ Global attention                                          â”‚
â”‚  â†’ Learns mid-level patterns (limb movements, short actions) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Higher Stages (Stage 2)                                     â”‚
â”‚  â€¢ Very few tokens (50)                                      â”‚
â”‚  â€¢ Large receptive fields (8Ã—1Ã—72)                           â”‚
â”‚  â€¢ Global attention                                          â”‚
â”‚  â†’ Learns high-level patterns (complex actions, activities)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Architecture Works

1. **Computational Efficiency**: Fewer tokens in later stages = faster attention
2. **Multi-scale Learning**: Different stages capture different temporal/spatial scales
3. **Hierarchical Abstraction**: Natural progression from details to concepts
4. **Information Bottleneck**: Forces model to learn meaningful representations

### Common Questions

**Q: Why reduce tokens so aggressively (4,800 â†’ 50)?**
A: This forces the model to compress information efficiently and learn hierarchical abstractions. The 50 final tokens must encode everything needed to reconstruct the input!

**Q: How does pooling differ from downsampling?**
A: Q-pooling is special: it only pools the queries in attention, not the keys/values. This allows attending to fine-grained information while producing coarser outputs.

**Q: What's the mask unit size (4, 1, 24)?**
A: Mask units are groups of tokens that are masked together. Size (4,1,24) means 4 temporal Ã— 1 height Ã— 24 width tokens = 96 tokens per mask unit. This prevents information leakage.

**Q: Why use different attention types (local vs global)?**
A: Stage 0 uses local attention for efficiency with many tokens. Later stages use global attention because there are fewer tokens (600, then 50), making global attention feasible.

### Performance Impact

```
Configuration Trade-offs:

Larger q_strides:
  âœ“ Faster computation (fewer tokens)
  âœ“ Larger receptive fields
  âœ— Less fine-grained control
  âœ— May lose local details

Smaller q_strides:
  âœ“ More fine-grained features
  âœ“ Better detail preservation
  âœ— More tokens = slower
  âœ— More memory usage

Your config (2,1,4;2,1,6) strikes a balance:
  â€¢ Good reduction (96Ã—) for efficiency
  â€¢ Maintains spatial width information (aggressive width pooling only at end)
  â€¢ Gradual temporal pooling (2Ã— at each stage)
```

### See Also

- ğŸ“˜ [Full Data Propagation Guide](data_propagation_guide.md) - Comprehensive explanation
- ğŸ¨ [Visual Diagrams](visual_data_flow.md) - ASCII art visualizations
- ğŸ“Š [Tensor Shapes](tensor_shapes_detailed.md) - Detailed shape transformations
- ğŸ”§ [Demo Script](demo_data_propagation.py) - Calculate for any config
- ğŸ  [Documentation Index](README.md) - All documentation

### Quick Command to Run Demo

```bash
cd docs
python demo_data_propagation.py
```

This will show you the exact data flow with your configuration!
